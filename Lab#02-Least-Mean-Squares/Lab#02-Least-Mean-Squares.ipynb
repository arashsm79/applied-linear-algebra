{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Linear Algebra - Lab 2\n",
    "Introduction to Linear regression, QR Descomposition and Multiple Linear regression\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [Projection matrices and least squares](#sec1)\n",
    "    * [Exercise 1](#sec2)\n",
    "* [Solving the least squares equation via QR Decomposition](#sec3)\n",
    "    * [Exercise 2](#sec4)\n",
    "\n",
    "* [Predicting Medical Cost with Linear regression](#sec5)\n",
    "    * [Data Preprocessing](#sec5_1)\n",
    "    * [Model building](#sec5_2)\n",
    "    * [Training the Model](#sec5_3)\n",
    "        * [Exercise 3](#sec5_4)\n",
    "    * [Model Evaluation](#sec5_5)\n",
    "        * [Exercise 4](#sec5_6)\n",
    "* [Optional Excersice](#sec6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection matrices and least squares <a class=\"anchor\" id=\"sec1\"></a>\n",
    "\n",
    "### Least square approximation\n",
    "A crucial application of least squares is fitting a straight line to $m$ points.\n",
    "\n",
    "Consider five points in the plane: \n",
    "\n",
    "$$ (x_i, y_{i}) = (1, 4), (2, 8), (4, 10) , (5, 12), (7, 18)$$\n",
    "\n",
    "- Find the closest line to these five points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1], [2], [4], [5], [7]]) \n",
    "y = np.array([[4], [8], [10], [12], [18]]) \n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No straight line $ y = \\theta_0 + \\theta_1 x  $ goes through these five points. \n",
    "- We are looking for numbers $\\theta_0$ and $\\theta_1 $that satisfy five equations:\n",
    "     \n",
    "$$ (x_1 = 1) \\;\\;\\;\\; y_1 = \\theta_0 + 1\\theta_1 = 4 $$\n",
    "     \n",
    "$$ (x_2 = 2) \\;\\;\\;\\; y_2 = \\theta_0 + 2\\theta_1 = 8 $$\n",
    "     \n",
    "$$ (x_3 = 4) \\;\\;\\;\\; y_3 = \\theta_0 + 4\\theta_1 = 10 $$\n",
    "\n",
    "$$ (x_4 = 5) \\;\\;\\;\\; y_4 = \\theta_0 + 5\\theta_1 = 12 $$\n",
    "\n",
    "$$ (x_5 = 7) \\;\\;\\;\\; y_5 = \\theta_0 + 7\\theta_1 = 18 $$\n",
    "    \n",
    "    \n",
    "- This 5 by 2 system has no solution, $ y = (4, 8, 10, 12, 18) $ is not a combination of the columns of $X$.\n",
    "\n",
    "$$X = \\begin {bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 4\\\\ 1 & 5 \\\\ 1 & 7  \\end{bmatrix} \\;\\;\\;\\; \\theta = \\begin {bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix} \\;\\;\\;\\; y = \\begin {bmatrix} 4 \\\\ 8 \\\\ 10\\\\ 12 \\\\18 \\end{bmatrix} \\;\\;\\;\\; X \\theta = y \\;\\; \\text{is not solvable} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing the Error\n",
    "\n",
    "- Now that we cannot fit a line that goes through all five points, we try to find the best line $(\\hat{\\theta})$ for the five points and minimize the overall error, the error is \n",
    "    $ | e |^2 = | y - X\\hat{\\theta} |^2 $\n",
    "    \n",
    "- In order to minimize the error, we look for the closest point to $y$ that is in the column space of $X$, the nearest point is $p$ (the projection of $b$ into $A$.)\n",
    "\n",
    "- Every vector $b$ splits into two parts, The part in the column space is $p$. and The perpendicular part is $e. \\; (y = p + e) $\n",
    "\n",
    "- We can find $\\hat{\\theta}$ (best fitting line) by solving the equation $ X^T X  \\hat{\\theta} = X^T y$\n",
    "\n",
    "$$ \\hat{\\theta} = \\begin {bmatrix} \\hat{\\theta_0} \\\\ \\hat{\\theta_1} \\end{bmatrix} \\;\\;\\;\\;\\;\\;\\; \\hat{\\theta} =  (X^T X)^{-1} X^T y $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 <a class=\"anchor\" id=\"sec2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1], [1, 2], [1, 4], [1, 5], [1, 7]]) \n",
    "y = np.array([[4], [8], [10], [12], [18]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** calculate $\\hat{\\theta}$ for the given data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "theta_hat = ...\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the best line that minimizes the overall error is \n",
    "\n",
    "$$h(x_i) = \\hat{\\theta_0} + \\hat{\\theta_1} \\times x_i = 2.33 + 2.12 \\times x_i \\;\\;\\;\\;\\;\\text{(this is the hypothesis function)} \\;\\; \\text{and} \\;\\;  h = X \\hat{\\theta} $$\n",
    "\n",
    "**Question 2:** calculate $h$ (matrix of predicted $y$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = ...\n",
    "print(h)\n",
    "#h[0], h[1], h[2], h[3] and h[4] are predicted points for y0, y1, y2, y3 and y4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.scatter(x, h, color = 'black')\n",
    "plt.plot(x, h, color = 'red')\n",
    "plt.show()\n",
    "#red line is the best line for that three points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with this approach is the matrix inverse that is both computationally expensive and numerically unstable. An alternative approach is to use a matrix decomposition to avoid this operation. We will look at QR decomposition in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the least squares equation via QR Decomposition <a class=\"anchor\" id=\"sec3\"></a>\n",
    "\n",
    "- The QR decomposition (also called the QR factorization) of a matrix is a decomposition of the matrix into an orthogonal matrix and a triangular matrix. \n",
    "$$ A = QR $$\n",
    "where $Q$ is an orthogonal matrix ($Q^T Q = I$) and $R$ is an upper triangular matrix. \n",
    "- $Q$ is a $m*n$ matrix and $R$ is an upper triangle matrix with the size $n * n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Orthogonal Matrix $Q$ with orthonormal columns satisfies $ Q^T Q = I$ :\n",
    "\n",
    "$$Q^T Q = \\begin {bmatrix}  & q_1^T & \\\\  & q_2^T  & \\\\ & q_3^T & \\end{bmatrix} \\begin {bmatrix}  &  & \\\\ q_1 & q_2 & q_3 \\\\ & & \\end{bmatrix} = \n",
    "\\begin {bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = I $$ \n",
    "There are several methods for computing the QR decomposition. One of such method is the Gram-Schmidt process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gram-Schmidt Process \n",
    "\n",
    "Start with the independent columns of $A$: $a_1, a_2, ..., a_n$. We want to construct orthogonal vectors $u_1, u_2, ..., u_n$. Then we divide $u_1, u_2, ..., u_n$ by their lengths.\n",
    "\n",
    "That produces orthonormal vectors $q_1 = \\frac{u_1}{||u_1||}, q_2 = \\frac{u_2}{||u_2||}, ..., q_n = \\frac{u_n}{||u_n||} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Begin by choosing $u_1 = a_1$. This first direction is accepted as it comes.\n",
    "The next direction $u_2$ must be perpendicular to $u_1$. Start with $a_2$ and subtract its projection along $u_1$. This leaves the perpendicular part, which is the orthogonal vector $u_2$:\n",
    "\n",
    "$$ \\text{First Gram-Schmidt step} \\;\\;\\;\\;\\;\\;\\;\\; u_2 = a_2 - \\frac{u_1^T a_2}{u_1^T u_1} u_1.$$\n",
    "\n",
    "so now $u_1$ and $u_2$ are orthogonal.\n",
    "The third direction starts with $a_3$. This is not a combination of $u_1$ and $u_2$ (because $a_3$ is not a combination of $a_1$ and $a_2$). But most likely $a_3$ is not perpendicular to $u_1$ and $u_2$. So subtract off its components in those two directions to get a perpendicular direction $u_3$:\n",
    "\n",
    "$$ \\text{Next Gram-Schmidt step} \\;\\;\\;\\;\\;\\;\\;\\; u_3 = a_3 - \\frac{u_1^T a_3}{u_1^T u_1} u_1 - \\frac{u_2^T a_3}{u_2^T u_2} u_2.$$\n",
    "\n",
    "This is the idea of the Gram-Schmidt process. Subtract from every new\n",
    "vector its projections in the directions already set. That idea is repeated at every step.\n",
    "For the fourth vector $a_4$, we would subtract three projections onto $u_1, u_2, u_3$ to get $u_4$.\n",
    "\n",
    "$$u_x = a_x - \\sum_{i=1}^{x-1} (\\frac{u_i^T . a_x}{u_i^T . u_i})\\; u_i \\;\\;\\;\\;\\; \\text{ for x = 2, ..., n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, or immediately when each one is found, divide the orthogonal vectors $u_1, u_2, ..., u_n$ by their lengths. The resulting vectors $q_1, q_2, ..., q_n$ are orthonormal.\n",
    "\n",
    "$$q_x = \\frac{u_x}{||u_x||} \\;\\;\\;\\;\\; \\text{ for x = 1, ..., n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q = \\begin {bmatrix} q_1 \\;|\\; q_2 \\;|\\; ... \\;|\\; q_n\\end{bmatrix} \\;\\;\\; \\text{q's are columns of Q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a matrix $A$ and ended up with a matrix $Q$. How are those matrices related? matrix $R$ connects them, $A = QR$\n",
    "\n",
    "For a $3$by $3$ matrix A: \n",
    "$$ A = \\begin {bmatrix}  &  & \\\\ a_1 & a_2  & a_3 \\\\ & & \\end{bmatrix} \n",
    "= QR\n",
    "= \\begin {bmatrix}  &  & \\\\ q_1 & q_2 & q_3 \\\\ & & \\end{bmatrix} \n",
    "\\begin {bmatrix} q_1^T a_1 & q_1^T a_2 & q_1^T a_3\\\\  & q_2^T a_2 & q_2^T a_3 \\\\ & & q_3^T a_3 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gram-Schmidt process \n",
    "## Exercise 2 <a class=\"anchor\" id=\"sec4\"></a>\n",
    "**Question 1:** implement the function `qr_gram_schmidt` which takes the matrix $A$ and returns the $Q$ and $R$ using gram schmidt process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qr_gram_schmidt(A):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using QR decomposition, the coefficients can be found as follows:\n",
    "$$ \\hat{\\theta}_{qr} = R^-1 \\; Q^T \\; y$$\n",
    "\n",
    "**Question 2:** calculate $\\hat{\\theta}_{qr}$ for $X$ and $y$ using QR decomposition and `qr_gram_schmidt` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q, r = ...\n",
    "theta_hat_qr = ...\n",
    "theta_hat_qr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get the same result by using QR decompositon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Medical Cost with Linear regression <a class=\"anchor\" id=\"sec5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to build a model to predict medical cost using Linear regression and least square approximation. we use Medical Cost Personal Dataset. this dataset consists of age, sex, bmi, children, smoker and region as __independent variables__ and charges as __dependent variable__.\n",
    "\n",
    "- age: age of primary beneficiary\n",
    "- sex: insurance contractor gender, female, male\n",
    "- bmi: Body mass index\n",
    "- children: Number of children covered by health insurance \n",
    "- smoker: Smoking\n",
    "- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
    "- charges: Individual medical costs billed by health insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example (that we tried to fit a line through some points), there was only one independent variable. the hypothesis function we used was as follows:\n",
    "$$ h(x_i) = \\hat{\\theta_0} + \\hat{\\theta_1} x_i : \\;\\;\\;\\;\n",
    "\\begin{bmatrix} 1&x_1\\\\1&x_2\\\\.&.\\\\.&.\\\\1&x_m\\end{bmatrix} \n",
    "\\begin{bmatrix}\\hat{\\theta_0}\\\\\\hat{\\theta_1}\\end{bmatrix} \n",
    "= \\begin{bmatrix} h(x_1)\\\\h(x_2)\\\\.\\\\.\\\\h(x_m)\\end{bmatrix} \n",
    "\\;\\;\\;\\;\\;\\; \\text{($x_i$ is indepedent variable and $y_i$ is dependent variable)} $$\n",
    "\n",
    "($h(x_i)$ is the predicted value of $y_i$)\n",
    "\n",
    "In this dataset we have multiple independent variables, so we use __Multiple linear regression__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('insurance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of dataset, there are $m=1338$ training examples and $n=7$ variables. Target variable here is __charges__. using multiple linear regression, hypothesis function looks like this: \n",
    "$$h(x_i) = age \\times \\theta_1 + sex \\times \\theta_2 + bmi \\times \\theta_3 + children \\times \\theta_4 + smoker \\times \\theta_5 + region \\times \\theta_6 \\;\\;\\; (\\text{for the $i^{th}$ training example})$$\n",
    "($h(x_i)$ is the predicted value of $i^{th} $training example, $\\theta_1, ..., \\theta_6$ are coefficants of hypothesis function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $i^{th}$ training example can be represented as:\n",
    "$$x_i =  \\begin{bmatrix} x_{i1}&x_{i2}&...&x_{i6} \\end{bmatrix}=  \\begin{bmatrix} age_{1}&sex_{2}&...&region_{6} \\end{bmatrix}$$\n",
    "now we combine all training examples into single input matrix of size(m, n):\n",
    "$$ \\mathbf{X}= \\left( \\begin{smallmatrix} x_{11} & x_{12} &.&.&.&.& x_{1n}\\\\\n",
    "                                x_{21} & x_{22} &.&.&.&.& x_{2n}\\\\\n",
    "                                x_{31} & x_{32} &.&.&.&.& x_{3n}\\\\\n",
    "                                .&.&.&. &.&.&.& \\\\\n",
    "                                .&.&.&. &.&.&.& \\\\\n",
    "                                x_{m1} & x_{m2} &.&.&.&.&. x_{mn}\\\\\n",
    "                                \\end{smallmatrix} \\right)_{(m,n)}$$\n",
    "We represent coefficients of function and dependent variable in vector form as  \n",
    "$$\\theta = \\left (\\begin{matrix} \\theta_1 \\\\ \\theta_2 \\\\ .\\\\.\\\\ \\theta_6 \\end {matrix}\\right)\\; , \\;\\;\n",
    "\\mathbf{ y } = \\left (\\begin{matrix} y_1\\\\ y_2\\\\. \\\\. \\\\ y_i \\\\. \\\\. \\\\ y_m \\end{matrix} \\right)$$\n",
    "\n",
    "So we represent hypothesis function in vectorize form $$\\mathbf{ h_\\theta{(x)} = X\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing<a class=\"anchor\" id=\"sec5_1\"></a>\n",
    "\n",
    "The hypothesis function can't work with categorical data directly, categorical data must be converted to numbers.\n",
    "\n",
    "__Label encoding__ is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.\n",
    "\n",
    "__One hot encoding__ is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
    "\n",
    "- for 'sex' and 'smoker' column we will apply Label Encoding as there are only 2 catagories\n",
    "- for 'region' we will apply OneHot Encoding as there are more than 2 catagories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using pandas get_dummies function we can apply Label encoding and OneHot encoding in line of code. \n",
    "\n",
    "# Label Encoding:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['smoker'] = le.fit_transform(df['smoker'])\n",
    "\n",
    "# OneHot Encoding:\n",
    "df_encode = pd.get_dummies(data = df, prefix = 'IsIn', prefix_sep='_',\n",
    "               columns = ['region'],\n",
    "               drop_first =False,\n",
    "              dtype='int8')\n",
    "\n",
    "df_encode.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we used one hot encoding, some dummy variables were added to the indpendent variables. Now the hypothesis function is as follows: \n",
    "\n",
    "$$h(x_i) =  age .\\theta_1 + sex .\\theta_2 + bmi  .\\theta_3 + children . \\theta_4 + smoker .\\theta_5 + \\\\\n",
    "\\text{IsIn_northeast} .\\theta_6 + \\text{IsIn_northwest} .\\theta_7 + \\text{IsIn_southeast} . \\theta_8 + \\text{IsIn_southwest} . \\theta_9 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all inputs are numeric, we begin to build our model.\n",
    "## Model building<a class=\"anchor\" id=\"sec5_2\"></a>\n",
    "\n",
    "First we have to build the following matrices:\n",
    "$$h_{(x)} =X\\theta \\;\\; \\rightarrow \\;\\;\\;\n",
    "\\begin{bmatrix}\n",
    "x_{11}&x_{12}&...&x_{19}\\\\\n",
    "x_{21}&x_{22}&...&x_{29}\\\\\n",
    ".&.&...&.\\\\\n",
    "x_{m1}&x_{m2}&...&x_{m9}\\\\\n",
    "\\end{bmatrix} \\;\\;\n",
    "\\begin{bmatrix}\\theta_1\\\\\\theta_2\\\\\\theta_3\\\\\\theta_4\\\\\\theta_5\\\\\\theta_6\\\\\\theta_7\\\\\\theta_8\\\\\\theta_9\\end{bmatrix}\n",
    "= \\begin{bmatrix} h(x_1)\\\\h(x_2)\\\\.\\\\h(x_m)\\end{bmatrix} \\;\\;,\\;\\; \n",
    "\\mathbf{ y } = \\begin{bmatrix} y_1\\\\y_2\\\\.\\\\y_m\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = ...\n",
    "y = ...\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data \n",
    "in order to evaluate our model, we should split the data into training examples and test examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model<a class=\"anchor\" id=\"sec5_3\"></a>\n",
    "\n",
    "We want to find vector of coefficients ($\\theta$), then use the coefficients to predict charges ($h(x)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3<a class=\"anchor\" id=\"sec5_4\"></a>\n",
    "\n",
    "Using QR decomposition, the coefficients can be found as follows:\n",
    "$$ \\theta = R^-1 \\; Q^T \\; y$$\n",
    "\n",
    "**Question 1:** calculate $Q$ and $R$ using the `qr_gram_schmidt` function, then calculate $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q, R = ...\n",
    "from numpy.linalg import inv\n",
    "theta = ...\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction from our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have $\\theta$, using our hypothesis function we can predict charges:\n",
    "$$h(x) = \\mathbf{X} \\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** calculate predicted charges both for training data and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_train_pred is the hypothesis function prediction of training examples\n",
    "y_train_pred = ...\n",
    "#y_test_pred is the hypothesis function prediction of test examples\n",
    "y_test_pred  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "fig, ax=plt.subplots(figsize=(15,6))\n",
    "sns.lineplot(x=np.arange(len(y_test)) , y=y_test, label='Actuals',color='blue',ax=ax)\n",
    "sns.lineplot(x=np.arange(len(y_test)), y=y_test_pred, label='Predictions',color='red',ax=ax)\n",
    "ax.set_title('Charges: Actuals vs Predictions')\n",
    "ax.set_ylabel('Charges')\n",
    "ax.set_xlabel('Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation<a class=\"anchor\" id=\"sec5_5\"></a>\n",
    "\n",
    "We predicted value for charges by using our model coefficients for test data set. Now we will compare the predicted value with actual value in test set. \n",
    "\n",
    "$\\mathbf{R^2}$ is statistical measure of how close data are to the fitted regression line. $\\mathbf{R^2}$ is always between 0 to 100%. 0% indicates that model explains none of the variability of the response data around it's mean. 100% indicates that model explains all the variablity of the response data around the mean.\n",
    "\n",
    "$$\\mathbf{R^2 = 1 - \\frac{SSE}{SST}}$$\n",
    "**SSE = Sum of Square Error**  \n",
    "**SST = Sum of Square Total**  \n",
    "$$\\mathbf{SSE = \\sum_{i=1}^{m}(h(x_i) - y_i)^2}$$\n",
    "$$\\mathbf{SST = \\sum_{i=1}^{m}(h(x_i) - \\bar{y}_i)^2} \\;\\;\\; \\text{($\\mathbf{\\bar{y}}$ is mean value of $\\mathbf{y}$)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: <a class=\"anchor\" id=\"sec5_6\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** calculate $R^2$ score of testing data using $\\mathbf{R^2 = 1 - \\frac{SSE}{SST}}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sse_test_data = ...\n",
    "sst_test_data = ...\n",
    "R_square_test_data = ...\n",
    "\n",
    "print(R_square_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $R^2$ score above 0.70 for our model is good enough for purpose of this homework and it fits our data test very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "A concern with multiple regression is overfitting; with a lot of predictors and a\n",
    "limited number of samples, random sampling fluctuations will allow some linear\n",
    "combination of the predictors to match the predictand perfectly over the limited\n",
    "samples we have, but the correlations will fall apart for a different set of samples.\n",
    "\n",
    "we can calculate $R^2$ score of training data set and compare it to $R^2$ score of testing data set and check if overfitting happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sse_train_data = ...\n",
    "sst_train_data = ...\n",
    "R_square_train_data = ...\n",
    "\n",
    "print(R_square_train_data)\n",
    "print(R_square_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Difference between $R^2$ score of training and testing data should not be drastic and overfitting should not happen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Excersice: <a class=\"anchor\" id=\"sec6\"></a>\n",
    "\n",
    "\n",
    "students are welcome to use other regression techniques to improve performance and build a model for this data set that predicts **Charges** more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This document was compiled, gathered and coded by the teaching assistant team and may be used only for educational purposes. The authors would like to thank the many projects and educational material that made their source code freely available on the internet, especially otter-grader that made the generation and sanitization of the notebook easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
